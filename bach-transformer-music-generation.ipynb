{"cells": [{"cell_type": "code", "metadata": {}, "source": ["# Transformer-based Bach Cello Suite music generation\n", "# Run this notebook in Google Colab for best experience.\n", "\n", "!pip install music21 tensorflow==2.15.0 --quiet\n", "\n", "import os\n", "import numpy as np\n", "import tensorflow as tf\n", "from tensorflow.keras import layers, Model\n", "from music21 import converter, instrument, note, chord, duration, stream\n", "from IPython.display import Audio, display\n", "from google.colab import files\n", "\n", "MIDI_DIR = \"/content/bach_cello_midi\"\n", "os.makedirs(MIDI_DIR, exist_ok=True)\n", "print(\"Upload Bach Cello Suite MIDI files into:\", MIDI_DIR)\n", "\n", "SEQ_LEN = 128\n", "BATCH_SIZE = 16\n", "EPOCHS = 10\n", "EMBED_DIM = 256\n", "NUM_HEADS = 4\n", "FF_DIM = 512\n", "NUM_LAYERS = 4\n", "\n", "def parse_midi_file(path):\n", "    midi = converter.parse(path)\n", "    parts = instrument.partitionByInstrument(midi)\n", "    if parts:\n", "        notes_to_parse = parts.parts[0].recurse()\n", "    else:\n", "        notes_to_parse = midi.flat.notes\n", "    seq = []\n", "    for elem in notes_to_parse:\n", "        if isinstance(elem, note.Note):\n", "            pitch = elem.pitch.nameWithOctave\n", "            dur = float(elem.duration.quarterLength)\n", "            seq.append((pitch, dur))\n", "        elif isinstance(elem, chord.Chord):\n", "            pitch = elem.root().nameWithOctave\n", "            dur = float(elem.duration.quarterLength)\n", "            seq.append((pitch, dur))\n", "    return seq\n", "\n", "all_sequences = []\n", "for fname in os.listdir(MIDI_DIR):\n", "    if not fname.lower().endswith((\".mid\", \".midi\")):\n", "        continue\n", "    path = os.path.join(MIDI_DIR, fname)\n", "    try:\n", "        seq = parse_midi_file(path)\n", "        if len(seq) > 0:\n", "            all_sequences.append(seq)\n", "            print(f\"Parsed {fname} with {len(seq)} events.\")\n", "    except Exception as e:\n", "        print(\"Error parsing\", fname, \":\", e)\n", "print(\"Total sequences loaded:\", len(all_sequences))\n", "\n", "all_notes = set()\n", "all_durs = set()\n", "for seq in all_sequences:\n", "    for pitch, dur in seq:\n", "        all_notes.add(pitch)\n", "        all_durs.add(round(dur, 2))\n", "\n", "all_notes = sorted(list(all_notes))\n", "all_durs = sorted(list(all_durs))\n", "\n", "START_NOTE = \"<START_NOTE>\"\n", "START_DUR = \"<START_DUR>\"\n", "PAD_TOKEN = \"<PAD>\"\n", "\n", "note_vocab = [PAD_TOKEN, START_NOTE] + all_notes\n", "dur_vocab = [PAD_TOKEN, START_DUR] + [str(d) for d in all_durs]\n", "\n", "note2idx = {n: i for i, n in enumerate(note_vocab)}\n", "idx2note = {i: n for n, i in note2idx.items()}\n", "dur2idx = {d: i for i, d in enumerate(dur_vocab)}\n", "idx2dur = {i: d for d, i in dur2idx.items()}\n", "\n", "NOTE_VOCAB_SIZE = len(note_vocab)\n", "dur_VOCAB_SIZE = len(dur_vocab)\n", "\n", "def encode_sequence(seq, max_len=SEQ_LEN):\n", "    note_ids = [note2idx[START_NOTE]]\n", "    dur_ids = [dur2idx[START_DUR]]\n", "    for pitch, dur in seq:\n", "        if pitch not in note2idx:\n", "            continue\n", "        dur_str = str(round(dur, 2))\n", "        if dur_str not in dur2idx:\n", "            continue\n", "        note_ids.append(note2idx[pitch])\n", "        dur_ids.append(dur2idx[dur_str])\n", "    note_ids = note_ids[:max_len]\n", "    dur_ids = dur_ids[:max_len]\n", "    while len(note_ids) < max_len:\n", "        note_ids.append(note2idx[PAD_TOKEN])\n", "        dur_ids.append(dur2idx[PAD_TOKEN])\n", "    return np.array(note_ids, dtype=np.int32), np.array(dur_ids, dtype=np.int32)\n", "\n", "encoded_notes = []\n", "encoded_durs = []\n", "for seq in all_sequences:\n", "    n_ids, d_ids = encode_sequence(seq, SEQ_LEN)\n", "    encoded_notes.append(n_ids)\n", "    encoded_durs.append(d_ids)\n", "encoded_notes = np.array(encoded_notes)\n", "encoded_durs = np.array(encoded_durs)\n", "\n", "def create_inputs_targets(notes, durs):\n", "    in_notes = notes[:, :-1]\n", "    in_durs = durs[:, :-1]\n", "    out_notes = notes[:, 1:]\n", "    out_durs = durs[:, 1:]\n", "    return (in_notes, in_durs), (out_notes, out_durs)\n", "\n", "(X_notes, X_durs), (Y_notes, Y_durs) = create_inputs_targets(encoded_notes, encoded_durs)\n", "dataset = tf.data.Dataset.from_tensor_slices(((X_notes, X_durs), (Y_notes, Y_durs)))\n", "dataset = dataset.shuffle(buffer_size=256).batch(BATCH_SIZE, drop_remainder=True)\n", "\n", "class TokenAndPositionEmbedding(layers.Layer):\n", "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n", "        super().__init__(**kwargs)\n", "        self.token_emb = layers.Embedding(vocab_size, embed_dim)\n", "        self.pos_emb = layers.Embedding(maxlen, embed_dim)\n", "        self.maxlen = maxlen\n", "    def call(self, x):\n", "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n", "        positions = self.pos_emb(positions)\n", "        x = self.token_emb(x)\n", "        return x + positions\n", "\n", "class TransformerBlock(layers.Layer):\n", "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n", "        super().__init__()\n", "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n", "        self.ffn = tf.keras.Sequential(\n", "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n", "        )\n", "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n", "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n", "        self.dropout1 = layers.Dropout(rate)\n", "        self.dropout2 = layers.Dropout(rate)\n", "    def call(self, inputs, training=False, mask=None):\n", "        attn_output = self.att(inputs, inputs, attention_mask=mask)\n", "        attn_output = self.dropout1(attn_output, training=training)\n", "        out1 = self.layernorm1(inputs + attn_output)\n", "        ffn_output = self.ffn(out1)\n", "        ffn_output = self.dropout2(ffn_output, training=training)\n", "        return self.layernorm2(out1 + ffn_output)\n", "\n", "def build_music_transformer(maxlen, note_vocab_size, dur_vocab_size, embed_dim, num_heads, ff_dim, num_layers):\n", "    notes_in = layers.Input(shape=(maxlen-1,), name=\"notes_in\")\n", "    durs_in = layers.Input(shape=(maxlen-1,), name=\"durs_in\")\n", "    note_emb_layer = TokenAndPositionEmbedding(maxlen-1, note_vocab_size, embed_dim)\n", "    dur_emb_layer = TokenAndPositionEmbedding(maxlen-1, dur_vocab_size, embed_dim)\n", "    note_emb = note_emb_layer(notes_in)\n", "    dur_emb = dur_emb_layer(durs_in)\n", "    x = layers.Concatenate(axis=-1)([note_emb, dur_emb])\n", "    x = layers.Dense(embed_dim)(x)\n", "    for _ in range(num_layers):\n", "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n", "    note_logits = layers.Dense(note_vocab_size, name=\"note_logits\")(x)\n", "    dur_logits = layers.Dense(dur_vocab_size, name=\"dur_logits\")(x)\n", "    model = Model(inputs=[notes_in, durs_in], outputs=[note_logits, dur_logits])\n", "    return model\n", "\n", "model = build_music_transformer(\n", "    maxlen=SEQ_LEN,\n", "    note_vocab_size=NOTE_VOCAB_SIZE,\n", "    dur_vocab_size=len(dur_vocab),\n", "    embed_dim=EMBED_DIM,\n", "    num_heads=NUM_HEADS,\n", "    ff_dim=FF_DIM,\n", "    num_layers=NUM_LAYERS,\n", ")\n", "\n", "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n", "model.compile(\n", "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n", "    loss={\"note_logits\": loss_fn, \"dur_logits\": loss_fn},\n", "    loss_weights={\"note_logits\": 1.0, \"dur_logits\": 1.0},\n", ")\n", "\n", "history = model.fit(dataset, epochs=EPOCHS)\n", "\n", "def top_k_sample(logits, k=8, temperature=0.85):\n", "    logits = logits / temperature\n", "    values, indices = tf.math.top_k(logits, k=k)\n", "    indices = indices.numpy()\n", "    values = values.numpy()\n", "    values = values - values.max()\n", "    probs = np.exp(values)\n", "    probs = probs / probs.sum()\n", "    idx = np.random.choice(indices, p=probs)\n", "    return int(idx)\n", "\n", "def generate_music(model, max_len=SEQ_LEN, seed_notes=None, seed_durs=None):\n", "    if seed_notes is None:\n", "        seed_notes = [note2idx[START_NOTE]]\n", "        seed_durs = [dur2idx[START_DUR]]\n", "    notes = seed_notes[:]\n", "    durs = seed_durs[:]\n", "    while len(notes) < max_len:\n", "        in_notes = np.array(notes[-(max_len-1):], dtype=np.int32)\n", "        in_durs = np.array(durs[-(max_len-1):], dtype=np.int32)\n", "        while len(in_notes) < (max_len-1):\n", "            in_notes = np.append(in_notes, note2idx[PAD_TOKEN])\n", "            in_durs = np.append(in_durs, dur2idx[PAD_TOKEN])\n", "        in_notes = in_notes[np.newaxis, :]\n", "        in_durs = in_durs[np.newaxis, :]\n", "        note_logits, dur_logits = model.predict([in_notes, in_durs], verbose=0)\n", "        note_logits_last = note_logits[0, -1, :]\n", "        dur_logits_last = dur_logits[0, -1, :]\n", "        next_note_id = top_k_sample(note_logits_last, k=8, temperature=0.85)\n", "        next_dur_id = top_k_sample(dur_logits_last, k=8, temperature=0.85)\n", "        if next_note_id == note2idx[PAD_TOKEN] and next_dur_id == dur2idx[PAD_TOKEN]:\n", "            break\n", "        notes.append(next_note_id)\n", "        durs.append(next_dur_id)\n", "    return notes, durs\n", "\n", "gen_notes_ids, gen_durs_ids = generate_music(model, max_len=SEQ_LEN)\n", "gen_sequence = []\n", "for nid, did in zip(gen_notes_ids, gen_durs_ids):\n", "    pitch = idx2note.get(nid, PAD_TOKEN)\n", "    dur_str = idx2dur.get(did, \"0.25\")\n", "    if pitch in (PAD_TOKEN, START_NOTE):\n", "        continue\n", "    if dur_str in (PAD_TOKEN, START_DUR):\n", "        continue\n", "    dur = float(dur_str)\n", "    gen_sequence.append((pitch, dur))\n", "print(\"Generated sequence (first 20 events):\")\n", "print(gen_sequence[:20])\n", "\n", "def sequence_to_midi(seq, out_path, tempo_bpm=120):\n", "    s = stream.Stream()\n", "    from music21 import tempo as m21tempo\n", "    s.append(m21tempo.MetronomeMark(number=tempo_bpm))\n", "    for pitch, dur in seq:\n", "        n = note.Note(pitch)\n", "        n.duration = duration.Duration(dur)\n", "        s.append(n)\n", "    s.write(\"midi\", fp=out_path)\n", "\n", "out_midi_path = \"/content/generated_bach_transformer_sample.mid\"\n", "sequence_to_midi(gen_sequence, out_midi_path)\n", "files.download(out_midi_path)\n"], "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}